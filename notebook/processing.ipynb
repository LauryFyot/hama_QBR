{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import chardet\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå D√©finir les dossiers\n",
    "DOSSIER_VENDOR = \"../source/vendor_central_mois\"\n",
    "DOSSIER_AMVISOR = \"../source/amvisor_mois\"\n",
    "DOSSIER_NET_PPM = \"../source/vendor_central_netppm\"\n",
    "DOSSIER_DATA = \"../data\"\n",
    "\n",
    "# Fonction pour extraire la date (YYYY-MM) d'un fichier\n",
    "def extraire_mois_annee(nom_fichier):\n",
    "    # Format AMVisor (YYYY-MM)\n",
    "    match_amvisor = re.search(r\"(\\d{4})-(\\d{2})\", nom_fichier)\n",
    "    if match_amvisor:\n",
    "        return match_amvisor.group(0)\n",
    "    \n",
    "    # Format Vendor Central (JJ-MM-YYYY_JJ-MM-YYYY)\n",
    "    match_vendor = re.search(r\"(\\d{2})-(\\d{2})-(\\d{4})_\\d{2}-\\d{2}-\\d{4}\", nom_fichier)\n",
    "    if match_vendor:\n",
    "        return f\"{match_vendor.group(3)}-{match_vendor.group(2)}\"  \n",
    "\n",
    "    return None\n",
    "\n",
    "# Fonction pour r√©cup√©rer les fichiers d'un mois donn√© et v√©rifier leur pr√©sence\n",
    "def get_files(annee_mois):\n",
    "    fichiers_vendor = os.listdir(DOSSIER_VENDOR)\n",
    "    fichiers_amvisor = os.listdir(DOSSIER_AMVISOR)\n",
    "    fichiers_net_ppm = os.listdir(DOSSIER_NET_PPM)\n",
    "\n",
    "    # Filtrer les fichiers correspondant au mois donn√©\n",
    "    fichiers_trouves = [\n",
    "        os.path.join(DOSSIER_AMVISOR, f) for f in fichiers_amvisor if extraire_mois_annee(f) == annee_mois\n",
    "    ] + [\n",
    "        os.path.join(DOSSIER_VENDOR, f) for f in fichiers_vendor if extraire_mois_annee(f) == annee_mois\n",
    "    ] + [\n",
    "        os.path.join(DOSSIER_NET_PPM, f) for f in fichiers_net_ppm if extraire_mois_annee(f) == annee_mois\n",
    "    ]\n",
    "\n",
    "    # V√©rification : on doit avoir exactement 3 fichiers (Export, Fabrication, Approvisionnement)\n",
    "    if len(fichiers_trouves) < 4:\n",
    "        print(f\"‚ùå Fichiers manquants pour {annee_mois} : trouv√©s {len(fichiers_trouves)}/3.\") # raise FileNotFoundError\n",
    "    if len(fichiers_trouves) == 2:\n",
    "        fichiers_trouves = fichiers_trouves\n",
    "\n",
    "    return fichiers_trouves\n",
    "    \n",
    "# Fonction de processing des fichiers Amvisor\n",
    "def process_amvisor(path):\n",
    "\n",
    "    # Get columns\n",
    "    columns_main = [\"ASIN\", 'Item no.', \"EAN\", \"Item\"]\n",
    "    columns_overall = [\"Extras\", \"Size\", \"Description Item\", \"Cat. 1\", \"Cat. 2\", \"Cat. 3\", \"Cat. 4\", \"Visible\", \"Title Content\", \"Variations\", \"Brand store URL\", \"Images\", \"Videos\", \"AI summary\", \"Code\", \"Catalogue\"]\n",
    "    columns_ads = [\"Ads Impressions CM\", \"Ads Clicks CM\", \"Ads CTR CM\", \"Ads Units 14d CM\", \"Ads Costs CM\", \"Ads RoAS CM\", \"Ads CVR CM\"]\n",
    "    columns_sales = [\"Sell-out CM\", \"Sell-out PM\", 'Total sell-out CM', 'Total sell-out PM', \"Units CM\", \"Units PM\", \"SRP\", \"Margin\", \"Replacements CM\", \"Replacements PM\"] #, \"Revenue CM\", \"Revenue PM\"\n",
    "    columns_stocks = ['Stock', 'Stock value', 'Total stock', 'Total stock value']\n",
    "    columns_state = [\"Reviews\", \"Avg. stars\", \"Stars\",'Coverage', 'Rank 1', 'Rank 2', 'Buy Box', 'Buy Box PM', 'Days not Buy Box', \"Views CM\", \"Views PM\", \"CVR CM\", \"CVR PM\"]\n",
    "    all_columns = columns_main + columns_overall + columns_ads + columns_sales + columns_stocks + columns_state\n",
    "\n",
    "    # Return empty DataFrame if no file found\n",
    "    if not path:\n",
    "        return pd.DataFrame(columns=all_columns)\n",
    "\n",
    "    # Load Amvisor\n",
    "    df = pd.read_csv(path, sep=None, engine='python', encoding=\"ISO-8859-1\", dtype={\"EAN\": str})\n",
    "    df = df.dropna(subset=[\"ASIN\"])\n",
    "    \n",
    "    # Transform columns\n",
    "    colonnes_a_convertir = columns_ads + columns_sales + columns_stocks + columns_state\n",
    "    colonnes_a_convertir = [col for col in colonnes_a_convertir if col != \"Buy Box\"]\n",
    "    df[colonnes_a_convertir] = df[colonnes_a_convertir].replace({\"‚Ç¨\": \"\", \"%\": \"\", \"\\u202f\": \"\", \",\": \".\"}, regex=True).astype(float)\n",
    "\n",
    "    # Select data\n",
    "    all_columns = [col for col in all_columns if col in df.columns]\n",
    "    df = df[all_columns]\n",
    "\n",
    "    return df\n",
    "\n",
    "# Fonction de processing des fichiers Vendor Central\n",
    "def process_vendor_central(path1, path2):\n",
    "\n",
    "    # Load data\n",
    "    df_fab = pd.read_csv(path1, skiprows=1)\n",
    "    df_app = pd.read_csv(path2, skiprows=1)\n",
    "\n",
    "    # Select columns\n",
    "    columns = [\"ASIN\", \"Nom du produit\", \"Marque\", \"COGS exp√©di√©\", \"COGS exp√©di√© ‚Äì P√©riode ant√©rieure (%)\", \"COGS exp√©di√© ‚Äì M√™me p√©riode l'ann√©e derni√®re (%)\", \"Unit√©s exp√©di√©es\", \"Unit√©s exp√©di√©es ‚Äì P√©riode ant√©rieure (%)\", \"Unit√©s exp√©di√©es ‚Äì M√™me p√©riode l'ann√©e derni√®re (%)\", \"Retours client\", \"Retours du client ‚Äì P√©riode ant√©rieure (%)\", \"Retours du client ‚Äì M√™me p√©riode l'ann√©e derni√®re (%)\"]\n",
    "    df_fab = df_fab[columns]\n",
    "    df_app = df_app[columns]\n",
    "    df_fab['fab_or_app'] = \"1_fab\"\n",
    "    df_app['fab_or_app'] = \"2_app\"\n",
    "\n",
    "    # Merge data\n",
    "    df = pd.concat([df_fab, df_app])\n",
    "\n",
    "    # Convert data\n",
    "    colonnes_a_convertir = [\n",
    "        \"COGS exp√©di√©\", \"COGS exp√©di√© ‚Äì P√©riode ant√©rieure (%)\", \"COGS exp√©di√© ‚Äì M√™me p√©riode l'ann√©e derni√®re (%)\",\n",
    "        \"Unit√©s exp√©di√©es\", \"Unit√©s exp√©di√©es ‚Äì P√©riode ant√©rieure (%)\", \"Unit√©s exp√©di√©es ‚Äì M√™me p√©riode l'ann√©e derni√®re (%)\",\n",
    "        \"Retours client\", \"Retours du client ‚Äì P√©riode ant√©rieure (%)\", \"Retours du client ‚Äì M√™me p√©riode l'ann√©e derni√®re (%)\"\n",
    "    ]\n",
    "    df[colonnes_a_convertir] = df[colonnes_a_convertir].replace({\"‚Ç¨\": \"\", \"%\": \"\", \"\\u202f\": \"\", \",\": \".\"}, regex=True).astype(float)\n",
    "\n",
    "    # Remove duplicates based on COGS\n",
    "    df = df.sort_values(by=[\"ASIN\", \"COGS exp√©di√©\", \"fab_or_app\"], ascending=[True, False, True])\n",
    "    df = df.drop_duplicates(subset=\"ASIN\", keep=\"first\")\n",
    "\n",
    "    # Rename columns\n",
    "    columns = [\"ASIN\", \"Nom du produit\", \"Marque\", \"COGS\", \"COGS LM (%)\", \"COGS SPLY (%)\", \"Unit√©s\", \"Unit√©s LM (%)\", \"Unit√©s SPLY (%)\", \"Retours\", \"Retours LM (%)\", \"Retours SPLY (%)\", \"Source (Fabrication ou Approvisionnement)\"]\n",
    "    df.columns = columns\n",
    "\n",
    "    # Compute before evols\n",
    "    # df[\"COGS LM\"] = (df[\"COGS\"] / (1 + df[\"COGS LM (%)\"] / 100)).round(2)\n",
    "    # df[\"COGS SPLY\"] = (df[\"COGS\"] / (1 + df[\"COGS SPLY (%)\"] / 100)).round(2)\n",
    "    # df[\"Unit√©s LM\"] = (df[\"Unit√©s\"] / (1 + df[\"Unit√©s LM (%)\"] / 100)).round().fillna(0).astype(int)\n",
    "    # df[\"Retours LM\"] = df[\"Retours\"] / (1 + df[\"Retours LM (%)\"] / 100)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Fonction de processing des fichiers Net PPM\n",
    "def process_net_ppm(path):\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(path, skiprows=1)\n",
    "\n",
    "    # Select columns\n",
    "    columns = [\"ASIN\", \"PPM nette\", \"PPM nette ‚Äì M√™me p√©riode l'ann√©e derni√®re (bps)\"]\n",
    "    df = df[columns]\n",
    "\n",
    "    # Convert data\n",
    "    colonnes_a_convertir = [\"PPM nette\", \"PPM nette ‚Äì M√™me p√©riode l'ann√©e derni√®re (bps)\"]\n",
    "    df[colonnes_a_convertir] = df[colonnes_a_convertir].replace({\"‚Ç¨\": \"\", \"%\": \"\", \"\\u202f\": \"\", \",\": \".\"}, regex=True).astype(float)\n",
    "\n",
    "    # Compute before evols\n",
    "    df[\"Net PPM SPLY\"] = df[\"PPM nette\"] / (1 + df[\"PPM nette ‚Äì M√™me p√©riode l'ann√©e derni√®re (bps)\"] / 10000)\n",
    "    df.drop(columns=[\"PPM nette ‚Äì M√™me p√©riode l'ann√©e derni√®re (bps)\"], inplace=True)\n",
    "\n",
    "    # Rename columns\n",
    "    columns = [\"ASIN\", \"Net PPM\", \"Net PPM SPLY\"]\n",
    "    df.columns = columns\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Fonction de merge des fichiers Vendor Central et Amvisor\n",
    "def process_amazon(df_amvisor, df_vendor_central, df_net_ppm):\n",
    "\n",
    "    df_amvisor['is_in_amvisor'] = True\n",
    "    df_vendor_central['is_in_vendor_central'] = True\n",
    "\n",
    "    # Merge data\n",
    "    df = pd.merge(df_vendor_central, df_amvisor, how=\"outer\", left_on=\"ASIN\", right_on=\"ASIN\")\n",
    "    print(f\"amvisor : {len(df_amvisor)} | vendor_central : {len(df_vendor_central)} | merged : {len(df)}\")\n",
    "\n",
    "    # Fill missing flags with False\n",
    "    df['is_in_amvisor'] = df['is_in_amvisor'].fillna(False)\n",
    "    df['is_in_vendor_central'] = df['is_in_vendor_central'].fillna(False)\n",
    "\n",
    "    # Merge with net ppm\n",
    "    df = pd.merge(df, df_net_ppm, how=\"left\", on=\"ASIN\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# Fonction de processing des fichiers Net PPM\n",
    "def process_stocks(path=\"../source/amvisor_stocks/hama_stock_20240101_20250331.csv\"):\n",
    "    print(\"-- Processing stocks ...\")\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(path, skiprows=0, low_memory=False)\n",
    "    cols_to_keep = ['ASIN'] + [col for col in df.columns if 'stock units' in col.lower()]\n",
    "    df = df[cols_to_keep]\n",
    "\n",
    "    # Clean columns: remove \\r\\n and split into 'date' and 'metric'\n",
    "    df.columns = df.columns.str.replace(r'\\r\\n', ' ', regex=True)\n",
    "\n",
    "    # Melt the DataFrame to long format\n",
    "    dt = df.melt(id_vars='ASIN', var_name='date_metric', value_name='value')\n",
    "\n",
    "    # Split date and metric\n",
    "    dt[['date', 'metric']] = dt['date_metric'].str.extract(r'(\\d{2}/\\d{2}/\\d{4}) (.*)')\n",
    "    dt['date'] = pd.to_datetime(dt['date'], format='%m/%d/%Y')\n",
    "\n",
    "    stock_units_df = dt[dt['metric'] == 'Stock units']\n",
    "    stock_units_df = stock_units_df[stock_units_df['ASIN'].notna()]\n",
    "    stock_units_df.rename(columns={\"date\": \"extract_date\"}, inplace=True)\n",
    "    stock_units_df = stock_units_df.pivot(index='ASIN', columns='extract_date', values='value')\n",
    "    stock_units_df = stock_units_df.dropna(subset=stock_units_df.columns.difference(['ASIN']), how='all')\n",
    "\n",
    "    total_stock_units_df = dt[dt['metric'] == 'Stock units']\n",
    "    total_stock_units_df = total_stock_units_df[total_stock_units_df['ASIN'].notna()]\n",
    "    total_stock_units_df.rename(columns={\"date\": \"extract_date\"}, inplace=True)\n",
    "    total_stock_units_df = total_stock_units_df.pivot(index='ASIN', columns='extract_date', values='value')\n",
    "    total_stock_units_df = total_stock_units_df.dropna(subset=total_stock_units_df.columns.difference(['ASIN']), how='all')\n",
    "\n",
    "    # Export to CSV\n",
    "    total_stock_units_df.to_csv(f\"{DOSSIER_DATA}/total_stock_units.csv\", index=False)\n",
    "    stock_units_df.to_csv(f\"{DOSSIER_DATA}/stock_units.csv\", index=False)\n",
    "\n",
    "    return stock_units_df, total_stock_units_df\n",
    "\n",
    "\n",
    "# Processing loop\n",
    "def processing(dates, DOSSIER_DATA):\n",
    "    all_data = []\n",
    "\n",
    "    for date in dates:\n",
    "        print(f\"-- Processing {date} ...\")\n",
    "\n",
    "        # Get files and process it\n",
    "        files = get_files(date)\n",
    "        fichier_amvisor = next((f for f in files if \"Export Hama\" in f), None)\n",
    "        fichier_fabrication = next((f for f in files if \"Fabrication\" in f), None)\n",
    "        fichier_approvisionnement = next((f for f in files if \"Approvisionnement\" in f), None)\n",
    "        fichier_net_ppm = next((f for f in files if \"PPM_nette\" in f), None)\n",
    "        amvisor = process_amvisor(fichier_amvisor)\n",
    "        vendor_central = process_vendor_central(fichier_fabrication,fichier_approvisionnement)\n",
    "        net_ppm = process_net_ppm(fichier_net_ppm)\n",
    "        data = process_amazon(amvisor, vendor_central, net_ppm)\n",
    "\n",
    "        data[\"extract_date\"] = date\n",
    "        data[\"annee\"] = data[\"extract_date\"].str[:4].astype(int)\n",
    "        data[\"mois\"] = data[\"extract_date\"].str[5:7].astype(int)\n",
    "        data.to_csv(f\"{DOSSIER_DATA}/data_{date}.csv\")\n",
    "        all_data.append(data)\n",
    "\n",
    "    # Concatenate all data\n",
    "    final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "    # Further computations\n",
    "\n",
    "    ## -- COGS SPLY\n",
    "    final_df[\"extract_date_LY\"] = pd.to_datetime(final_df[\"extract_date\"], format=\"%Y-%m\").dt.to_period(\"M\") + 12\n",
    "    dt = final_df[[\"ASIN\", \"extract_date_LY\", \"COGS\"]].copy()\n",
    "    dt[\"extract_date_LY\"] = dt[\"extract_date_LY\"].astype(str)\n",
    "    dt.rename(columns={\"COGS\": \"COGS SPLY (‚Ç¨)\", \"extract_date_LY\": \"extract_date\"}, inplace=True)\n",
    "    final_df = final_df.merge(dt, on=[\"ASIN\", \"extract_date\"], how=\"left\")\n",
    "    final_df = final_df.drop(columns=[\"extract_date_LY\"])\n",
    "    ## -- COGS diff SPLY\n",
    "    final_df[\"COGS diff SPLY (‚Ç¨)\"] = final_df[\"COGS\"] - final_df[\"COGS SPLY (‚Ç¨)\"]\n",
    "\n",
    "    ## -- New categories\n",
    "    new_cat = pd.read_excel(\"../source/adhoc/Sales part Cross reference IFS   AMAZON vers3 sans les doublons au 18.05.25 avec categories DE .xlsx\")\n",
    "    new_cat = new_cat[['Ref AMAZON', 'Cat DE 1 ', \"Cat DE 2\"]]\n",
    "    new_cat.columns = ['ASIN', 'Category 1', \"Category 2\"]\n",
    "    final_df = final_df.merge(new_cat, on=[\"ASIN\"], how=\"left\")\n",
    "\n",
    "    process_stocks()\n",
    "    \n",
    "    \n",
    "    final_df.to_csv(f\"{DOSSIER_DATA}/all_data.csv\", index=False)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Processing 2025-03 ...\n",
      "amvisor : 2715 | vendor_central : 1274 | merged : 2981\n",
      "-- Processing 2025-02 ...\n",
      "amvisor : 2583 | vendor_central : 1252 | merged : 2834\n",
      "-- Processing 2025-01 ...\n",
      "amvisor : 2225 | vendor_central : 1294 | merged : 2533\n",
      "-- Processing 2024-12 ...\n",
      "amvisor : 2234 | vendor_central : 1279 | merged : 2532\n",
      "-- Processing 2024-11 ...\n",
      "amvisor : 2251 | vendor_central : 1269 | merged : 2532\n",
      "-- Processing 2024-10 ...\n",
      "amvisor : 2205 | vendor_central : 1270 | merged : 2494\n",
      "-- Processing 2024-09 ...\n",
      "‚ùå Fichiers manquants pour 2024-09 : trouv√©s 3/3.\n",
      "amvisor : 0 | vendor_central : 1254 | merged : 1254\n",
      "-- Processing 2024-08 ...\n",
      "amvisor : 2286 | vendor_central : 1307 | merged : 2562\n",
      "-- Processing 2024-07 ...\n",
      "amvisor : 2313 | vendor_central : 1271 | merged : 2550\n",
      "-- Processing 2024-06 ...\n",
      "amvisor : 2302 | vendor_central : 1313 | merged : 2566\n",
      "-- Processing 2024-05 ...\n",
      "amvisor : 2240 | vendor_central : 1330 | merged : 2522\n",
      "-- Processing 2024-04 ...\n",
      "amvisor : 2141 | vendor_central : 1353 | merged : 2464\n",
      "-- Processing 2024-03 ...\n",
      "‚ùå Fichiers manquants pour 2024-03 : trouv√©s 3/3.\n",
      "amvisor : 0 | vendor_central : 1411 | merged : 1411\n",
      "-- Processing 2024-02 ...\n",
      "‚ùå Fichiers manquants pour 2024-02 : trouv√©s 3/3.\n",
      "amvisor : 0 | vendor_central : 1444 | merged : 1444\n",
      "-- Processing 2024-01 ...\n",
      "‚ùå Fichiers manquants pour 2024-01 : trouv√©s 3/3.\n",
      "amvisor : 0 | vendor_central : 1468 | merged : 1468\n",
      "-- Processing stocks ...\n"
     ]
    }
   ],
   "source": [
    "dates = [\"2025-03\", \"2025-02\", \"2025-01\", \"2024-12\",\"2024-11\",\"2024-10\",\"2024-09\",\"2024-08\",\"2024-07\",\"2024-06\",\"2024-05\",\"2024-04\",\"2024-03\",\"2024-02\",\"2024-01\"]\n",
    "data = processing(dates, DOSSIER_DATA)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
